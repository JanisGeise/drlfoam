"""

"""
import os
import sys
import argparse
import torch as pt

from typing import Tuple
from os.path import join

from drlfoam.agent import PPOAgent
from drlfoam.agent.agent import compute_gae
from drlfoam.constants import EPS_SP
from drlfoam.environment.env_model_rotating_cylinder import denormalize_data, create_slurm_config

BASE_PATH = os.environ.get("DRL_BASE", "")
sys.path.insert(0, BASE_PATH)


def check_trajectories(cl: pt.Tensor, cd: pt.Tensor, actions: pt.Tensor, alpha: pt.Tensor, beta: pt.Tensor) -> Tuple:
    """
    check the model-generated trajectories wrt realistic values or nan's.

    Note: these boundaries depend on the current setup, e.g. Reynolds number and therefore may have to be modified

    :param cl: trajectory of cl
    :param cd: trajectory of cd
    :param actions: trajectory of actions
    :param alpha: trajectory of alpha
    :param beta: trajectory of beta
    :return: status if trajectory is valid or not and which parameter caused the issue as tuple: (status, param)
    """
    status = (True, None)
    if (pt.max(cl.abs()).item() > 1.3) or (pt.isnan(cl).any().item()):
        status = (False, "cl")
    elif (pt.max(cd).item() > 3.5) or (pt.min(cd).item() < 2.85) or (pt.isnan(cd).any().item()):
        status = (False, "cd")
    elif (pt.max(actions.abs()).item() > 5.0) or (pt.isnan(actions).any().item()):
        status = (False, "actions")
    elif (pt.max(alpha.abs()).item() > 5e3) or (pt.isnan(alpha).any().item()):
        status = (False, "alpha")
    elif (pt.max(beta.abs()).item() > 5e3) or (pt.isnan(beta).any().item()):
        status = (False, "beta")

    return status


def predict_trajectories(env_model: list, episode: int, path: str, states: pt.Tensor, cd: pt.Tensor, cl: pt.Tensor,
                         actions: pt.Tensor, alpha: pt.Tensor, beta: pt.Tensor, n_probes: int, n_input_steps: int,
                         min_max: dict, len_trajectory: int = 400, model_no: int = None) -> dict and Tuple:
    """
    predict a trajectory based on a given initial state and action using trained environment models for cd, and cl-p

    :param env_model: list containing the trained environment model ensemble
    :param episode: the current episode of the training
    :param path: path to the directory where the training is currently running
    :param states: pressure at probe locations sampled from trajectories generated by within CFD used as initial states
    :param cd: cd sampled from trajectories generated by within CFD used as initial states
    :param cl: cl sampled from trajectories generated by within CFD used as initial states
    :param actions: actions sampled from trajectories generated by within CFD used as initial states
    :param alpha: alpha values sampled from trajectories generated by within CFD used as initial states
    :param beta: beta values sampled from trajectories generated by within CFD used as initial states
    :param n_probes: number of probes places in the flow field
    :param n_input_steps: number as input time steps for the environment models
    :param min_max: the min- / max-values used for scaling the trajectories to the intervall [0, 1]
    :param len_trajectory: length of the trajectory, 1sec CFD = 100 epochs
    :param model_no: index of the model used for predictions, if None then each step a model is randomly chosen
    :return: the predicted trajectory and a tuple containing the status if the generated trajectory is within realistic
             bounds, and if status = False which parameter is out of bounds
    """

    # test model: loop over all test data and predict the trajectories based on given initial state and actions
    # for each model of the ensemble: load the current state dict
    for model in range(len(env_model)):
        env_model[model].load_state_dict(pt.load(join(path, "env_model", f"bestModel_no{model}_val.pt")))

    # load current policy network (saved at the end of the previous episode)
    policy_model = (pt.jit.load(open(join(path, f"policy_trace_{episode - 1}.pt"), "rb"))).eval()

    # use batch for prediction, because batch normalization only works for batch size > 1
    # -> at least 2 trajectories required
    batch_size, dev = 2, "cuda" if pt.cuda.is_available() else "cpu"
    shape = (batch_size, len_trajectory)
    traj_cd, traj_cl, traj_alpha, traj_beta, traj_actions, traj_p = pt.zeros(shape).to(dev), pt.zeros(shape).to(dev), \
                                                                    pt.zeros(shape).to(dev), pt.zeros(shape).to(dev), \
                                                                    pt.zeros(shape).to(dev), \
                                                                    pt.zeros((batch_size, len_trajectory, n_probes)).to(dev)
    for i in range(batch_size):
        traj_cd[i, :n_input_steps] = cd
        traj_cl[i, :n_input_steps] = cl
        traj_alpha[i, :n_input_steps] = alpha
        traj_beta[i, :n_input_steps] = beta
        traj_actions[i, :n_input_steps] = actions
        traj_p[i, :n_input_steps, :] = states

    # loop over the trajectory, each iteration move input window by one time step
    for t in range(len_trajectory - n_input_steps):
        # create the feature (same for both environment models)
        feature = pt.flatten(pt.concat([traj_p[:, t:t + n_input_steps, :],
                                        (traj_cl[:, t:t + n_input_steps]).reshape([batch_size, n_input_steps, 1]),
                                        traj_cd[:, t:t + n_input_steps].reshape([batch_size, n_input_steps, 1]),
                                        (traj_actions[:, t:t + n_input_steps]).reshape([batch_size, n_input_steps, 1])],
                                       dim=2),
                             start_dim=1).to(dev)

        if model_no is None:
            # randomly choose an environment model to make a prediction if no model is specified
            tmp_env_model = env_model[pt.randint(low=0, high=len(env_model), size=(1, 1)).item()]
        else:
            tmp_env_model = env_model[model_no]

        # make prediction for probes, cl, and cd
        prediction = tmp_env_model(feature).squeeze().detach()
        traj_p[:, t + n_input_steps, :] = prediction[:, :n_probes]
        traj_cl[:, t + n_input_steps] = prediction[:, -2]
        traj_cd[:, t + n_input_steps] = prediction[:, -1]

        # use predicted (new) state to get an action for both environment models as new input
        # note: policy network uses real states as input (not scaled to [0, 1]), policy training currently on cpu
        s_real = denormalize_data(traj_p[:, t + n_input_steps, :], min_max["states"])
        tmp_pred = policy_model(s_real.to("cpu")).squeeze().detach()
        traj_alpha[:, t + n_input_steps], traj_beta[:, t + n_input_steps] = tmp_pred[:, 0], tmp_pred[:, 1]

        # sample the value for omega (scaled to [0, 1])
        beta_distr = pt.distributions.beta.Beta(traj_alpha[:, t + n_input_steps], traj_beta[:, t + n_input_steps])
        traj_actions[:, t + n_input_steps] = beta_distr.sample()

    # re-scale everything for PPO-training and sort into dict, therefore always use the first trajectory in the batch
    act_rescaled = denormalize_data(traj_actions, min_max["actions"])[0, :].to("cpu")
    cl_rescaled = denormalize_data(traj_cl, min_max["cl"])[0, :].to("cpu")
    cd_rescaled = denormalize_data(traj_cd, min_max["cd"])[0, :].to("cpu")
    p_rescaled = denormalize_data(traj_p, min_max["states"])[0, :, :].to("cpu")

    # sanity check if the created trajectories make sense
    status = check_trajectories(cl=cl_rescaled, cd=cd_rescaled, actions=act_rescaled, alpha=traj_alpha[0, :],
                                beta=traj_beta[0, :])

    # TODO: add reward fct for fluidic pinball -> choose reward fct based on current environment
    output = {"states": p_rescaled, "cl": cl_rescaled, "cd": cd_rescaled, "alpha": traj_alpha[0, :].to("cpu"),
              "beta": traj_beta[0, :].to("cpu"), "actions": act_rescaled, "generated_by": "env_models",
              "rewards": 3.0 - (cd_rescaled + 0.1 * cl_rescaled.abs())}

    return output, status


def assess_model_performance(s_model: list, a_model: list, r_model: list, agent: PPOAgent) -> list:
    """
    computes the policy loss of the current MB-episode for each model in the ensemble

    :param s_model: predicted states by each environment model
    :param a_model: actions predicted by policy network for each environment model
    :param r_model: predicted rewards by each environment model
    :param agent: PPO-agent
    :return: policy loss wrt environment models
    """
    policy_loss = []

    # assess the policy loss for each model
    for m in range(a_model[0].size()[-1]):
        values = [agent._value(s[:, :, m]) for s in s_model]

        log_p_old = pt.cat([agent._policy.predict(s[:-1, :, m], a[:-1, m])[0] for s, a in zip(s_model, a_model)])
        gaes = pt.cat([compute_gae(r[:, m], v, agent._gamma, agent._lam) for r, v in zip(r_model, values)])
        gaes = (gaes - gaes.mean()) / (gaes.std() + EPS_SP)

        states_wf = pt.cat([s[:-1, :, m] for s in s_model])
        actions_wf = pt.cat([a[:-1, m] for a in a_model])
        log_p_new, entropy = agent._policy.predict(states_wf, actions_wf)
        p_ratio = (log_p_new - log_p_old).exp()
        policy_objective = gaes * p_ratio
        policy_objective_clipped = gaes * p_ratio.clamp(1.0 - agent._policy_clip, 1.0 + agent._policy_clip)
        policy_loss.append(-pt.min(policy_objective, policy_objective_clipped).mean().item())

    return policy_loss


def fill_buffer_from_models(env_model: list, episode: int, path: str, observation: dict, n_input: int, n_probes: int,
                            buffer_size: int, len_traj: int, agent: PPOAgent, env: str = "local") -> Tuple[list, list]:
    """
    creates trajectories using data from the CFD environment as initial states and the previously trained environment
    models in order to fill the buffer

    :param env_model: list with all trained environment models
    :param episode: the current episode of the training
    :param path: path to the directory where the training is currently running
    :param observation: the trajectories sampled in CFD, used to sample input states for generating the trajectories
    :param n_input: number as input time steps for the environment models
    :param n_probes: number of probes places in the flow field
    :param buffer_size: size of the buffer, specified in args when running the run_training.py
    :param len_traj: length of the trajectory, 1sec CFD = 100 epochs
    :param agent: PPO-agent
    :param env: environment, either 'local' or 'slurm', is set in 'run_training.py'
    :return: a list with the length of the buffer size containing the generated trajectories
    """
    predictions, shape = [], (len_traj, len(env_model))
    r_model_tmp, a_model_tmp, s_model_tmp = pt.zeros(shape), pt.zeros(shape), pt.zeros((shape[0], n_probes, shape[1]))
    r_model, a_model, s_model = [], [], []

    # min- / max-values used for normalization
    min_max = {"states": observation["min_max_states"], "cl": observation["min_max_cl"],
               "cd": observation["min_max_cd"], "actions": observation["min_max_actions"]}

    counter, failed, max_iter = 0, 0, 50
    while counter < buffer_size:
        print(f"start filling buffer with trajectory {counter + 1}/{buffer_size} using environment models")

        # for each trajectory sample input states from all available data within the CFD buffer
        no = pt.randint(low=0, high=observation["cd"].size()[1], size=(1, 1)).item()

        # then predict the trajectory (the env. models are loaded in predict trajectory function)
        # TODO start: parallelize predictions when running training on cluster
        #               1. save obs
        #               2. each proc predicts traj -> param 'no' for sampling init states
        #               3. if traj valid then compute r_model_tmp, ...
        #               4. else save invalid traj & 'ok' param
        #               5. read in all the data once all procs finished -> if valid: append pred, *_model_tmp, else:
        #               6. count all invalid traj, print the info to log, set failed = N_discarded_traj
        #               7. start N_discard new procs and repeat until enough traj generated or failed >= max_iter
        #               8. either go back to CFD or asses model_performance
        pred, ok = predict_trajectories(env_model, episode, path, observation["states"][:, :, no],
                                        observation["cd"][:, no], observation["cl"][:, no],
                                        observation["actions"][:, no], observation["alpha"][:, no],
                                        observation["beta"][:, no], n_probes, n_input, min_max, len_traj)

        # only add trajectory to buffer if the values make sense, otherwise discard it
        if ok[0]:
            predictions.append(pred)

            # compute the uncertainty of the predictions for the rewards wrt the model number
            for model in range(len(env_model)):
                tmp, _ = predict_trajectories(env_model, episode, path,
                                              observation["states"][:, :, no], observation["cd"][:, no],
                                              observation["cl"][:, no], observation["actions"][:, no],
                                              observation["alpha"][:, no], observation["beta"][:, no],
                                              n_probes, n_input, min_max, len_traj, model_no=model)
                r_model_tmp[:, model] = tmp["rewards"]
                a_model_tmp[:, model] = tmp["actions"]
                s_model_tmp[:, :, model] = tmp["states"]

            # same data structure as required in update-method of PPO-agent
            r_model.append(r_model_tmp)
            a_model.append(a_model_tmp)
            s_model.append(s_model_tmp)

            counter += 1
            failed = 0

            # TODO end

        else:
            print_trajectory_info(counter, buffer_size, failed, pred, ok[1])
            failed += 1

        # if all the trajectories are invalid, abort training in order to avoid getting stuck in while-loop forever
        if failed >= max_iter:
            print(f"could not generate valid trajectories after {max_iter} iterations... going back to CFD")
            counter = buffer_size

    # compute the policy performance for each model in the current episode
    policy_loss_model = assess_model_performance(s_model, a_model, r_model, agent)

    return predictions, policy_loss_model


def print_trajectory_info(no: int, buffer_size: int, i: int, tra: dict, key: str) -> None:
    """
    if an invalid trajectory was generated this functions prints info's about the parameter which is out of bounds as
    well as min-/ max- and mean value of this parameter within the trajectory

    :param no: trajectory number wrt buffer size
    :param buffer_size: buffer size
    :param i: number of trails performed to generate the trajectory
    :param tra: the trajectory
    :param key: parameter which caused the out-of-bounds issue
    :return: None
    """
    vals = [(k, pt.min(tra[k]).item(), pt.max(tra[k]).item(), pt.mean(tra[k]).item()) for k in tra if k == key]
    print(f"\ndiscarding trajectory {no + 1}/{buffer_size} due to invalid values [try no. {i}]:")
    for val in vals:
        print(f"\tmin / max / mean {val[0]}: {round(val[1], 5)}, {round(val[2], 5)}, {round(val[3], 5)}")


def execute_prediction_slurm(traj_no: int, train_path: str = "examples/run_training/") -> None:
    """
    executes the model training on an HPC cluster using the singularity container

    :param traj_no: number of the trajectory containing the initial states
    :param train_path: path to current PPO-training directory
    :return: None
    """
    # cwd = 'drlfoam/drlfoam/environment/', so go back to the training directory
    os.chdir(join("..", "..", "examples"))

    # update path (relative path not working on cluster)
    train_path = join(BASE_PATH, "examples", train_path)

    settings = pt.load(join(train_path, "settings_prediction.pt"))
    trajectories = pt.load(join(train_path, "trajectories_prediction.pt"))


if __name__ == "__main__":
    ag = argparse.ArgumentParser()
    ag.add_argument("-n", "--number", required=True, help="number of the trajectory containing the initial states")
    args = ag.parse_args()
    execute_prediction_slurm(int(args.number), args.path)
