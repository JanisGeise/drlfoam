"""
executer for model prediction
"""
import sys
import logging
import torch as pt

from glob import glob
from os.path import join
from typing import Tuple
from os import chdir, environ, remove, getcwd

BASE_PATH = environ.get("DRL_BASE", "")
sys.path.insert(0, BASE_PATH)

from ...agent import PPOAgent
from ...constants import DEFAULT_TENSOR_TYPE
from ...execution import SlurmConfig
from ...execution.manager import TaskManager
from ...execution.slurm import submit_and_wait

from .check_trajectories import CheckTrajectories
from .predict_trajectories import PredictTrajectories
from .assess_model_performance import AssessModelPerformance

logger = logging.getLogger(__name__)
pt.set_default_tensor_type(DEFAULT_TENSOR_TYPE)


class ExecuteModelPrediction:
    def __init__(self, predict_path: str, executer: str, n_models: int, agent: PPOAgent, buffer_size: int,
                 trajectory_len: int, n_t_input: int, simulation, config: SlurmConfig = None, n_runner: int = None,
                 keys: list = None, seed: int = 0):
        """
        implements a class for handling the prediction by the environment models

        :param predict_path: path to the training directory
        :param executer: either 'local' or 'slurm'
        :param n_models: number of environment models
        :param agent: PPO agent
        :param buffer_size: buffer size
        :param trajectory_len: trajectory length
        :param n_t_input: number of initial time steps used as starting point to predict the trajectories
        :param simulation: environment class from drlfoam.environment, e.g., RotatingCylinder2D or RotatingPinball2D
        :param config: SLURM config if training is executed on an HPC cluster
        :param n_runner: number of runners if training is executed on an HPC cluster
        :param seed: seed value for choosing the starting points
        :param keys: dict keys from the dataloader class to ensure consistency
        """
        self._predict_path = predict_path
        self._executer = executer
        self._n_models = n_models
        self._agent = agent
        self._len_trajectory = trajectory_len
        self._buffer_size = buffer_size
        self._n_t_input = n_t_input
        self._sim = simulation
        self._config = config
        self._script_name = "execute_prediction.sh"
        self._manager = TaskManager(n_runners_max=n_runner)
        self._n_states = None
        self._n_actions = None

        self._check_performance = AssessModelPerformance(self._agent)
        self.check_mb_trajectories = CheckTrajectories(self._buffer_size, keys)
        self._max_iter = 50
        self._failed_total = 0
        self._n_failed_per_iter = 0
        self._counter = 0
        self._predictions = []
        self._policy_loss_model = []
        self._model_ensemble = []
        self._initial_states = None
        self._min_max = None
        self._episode = None
        self.actions_model = []
        self.states_model = []
        self.rewards_model = []

        # instantiate a class for executing the predictions if env is local, if SLURM the executer is set directly in
        # the prediction script
        self._predict = None if self._executer == "slurm" else PredictTrajectories(self._predict_path, self._n_t_input,
                                                                                   self._len_trajectory, self._n_models)

        # ensure reproducibility, for the prediction the seed value for each model differs and is set in
        # 'predict_trajectories.py'
        pt.manual_seed(seed)
        if pt.cuda.is_available():
            pt.cuda.manual_seed_all(seed)

    def execute(self, episode: int, model_ensemble: list, initial_states: dict,
                min_max_values: dict) -> Tuple[list, list]:
        """
        fill the model buffer with trajectories generated by the environment models

        :param episode: current episode
        :param model_ensemble: trained model ensemble
        :param initial_states: starting values from which the trajectory is predicted
        :param min_max_values: min. and max. values used for normalization within the dataloader class
        :return: policy loss and predicted trajectories
        """
        self._model_ensemble = model_ensemble
        self._initial_states = initial_states
        self._min_max = min_max_values
        self._episode = episode
        self._n_states = self._initial_states["states"].size(-1)
        self._n_actions = self._initial_states["actions"].size(-1)

        # reset the buffer and model performances before starting new predictions
        self._reset()

        # save the data for the prediction if we execute on an HPC
        if self._config is not None:
            self._save_for_execution_slurm()

        # create trajectories until the model buffer is filled
        while self._counter < self._buffer_size:
            logger.info(f"Creating trajectory {self._counter + 1}/{self._buffer_size} for the model buffer.")

            # for each trajectory sample input states from all available data within the CFD buffer
            no = pt.randint(low=0, high=self._initial_states["cx"].size(1), size=(1, 1)).item()

            # predict
            self._execute_prediction_local(no) if self._executer == "local" else self._execute_prediction_slurm(no)

            # if all the trajectories are invalid, abort training to avoid getting stuck in while-loop forever
            if self._failed_total >= self._max_iter:
                logger.warning(f"Could only generate {len(self._predictions)} valid trajectories after "
                               f"{self._max_iter} iterations.")
                break

        if self._failed_total < self._max_iter:
            # compute the policy performance for each model in the current episode if all trajectories are valid
            self._policy_loss_model = self._check_performance.check(self.states_model, self.actions_model,
                                                                    self.rewards_model)
        else:
            # else return to CFD, the policy loss usually in the order of 1e-17, so 1 is enough to trigger switching
            self._policy_loss_model = [1 for _ in range(self._n_models)]

        # reset failed_total
        self._failed_total = 0
        self._n_failed_per_iter = 0
        self._counter = 0

        # remove tmp data and scripts created when executed on an HPC
        if self._config is not None:
            self._reset_slurm()

        return self._policy_loss_model, self._predictions

    def _execute_prediction_local(self, no) -> None:
        """
        execute the prediction of trajectories when executer is local

        :param no: initial states trajectory number
        :return: None
        """
        # predict the trajectory (the env. models are loaded in predict trajectory function)
        pred = self._predict.predict(self._model_ensemble, self._initial_states, self._min_max, self._episode, no,
                                     self._n_states, self._n_actions)

        # only add trajectory to buffer if the values make sense, otherwise discard it
        if self.check_mb_trajectories.check_trajectories(pred):
            self._predictions.append(self._resort_predictions(pred))

            # compute the uncertainty of the predictions for the rewards wrt the model number
            pred_model = self._predict.predict_for_each_model(self._model_ensemble, self._initial_states, self._min_max,
                                                              self._n_actions, self._n_states, self._episode, no)
            pred_model = [self._resort_predictions(p) for p in pred_model]

            self.states_model.append(pt.cat([i["states"].unsqueeze(-1) for i in pred_model], dim=-1))
            self.actions_model.append(pt.cat([i["actions"].unsqueeze(-1) for i in pred_model], dim=-1))
            self.rewards_model.append(pt.cat([self._sim.reward(i).unsqueeze(-1) for i in pred_model], dim=-1))

            self._counter += 1
            self._failed_total = 0
            self.check_mb_trajectories.reset()

        else:
            self._failed_total += 1
            self.check_mb_trajectories.print_info(self._failed_total, self._counter)

    def _execute_prediction_slurm(self, no) -> None:
        """
        execute the prediction of trajectories when executer is slurm

        :param no: initial states trajectory number
        :return: None
        """
        # in the first iteration, we need to generate N_buffer trajectories
        if self._counter == 0:
            n_pred = self._buffer_size

        # in case some trajectories are invalid, we need to generate n_failed_per_iter trajectories more
        else:
            n_pred = self._n_failed_per_iter
            self._n_failed_per_iter = 0

        # make sure we are in the correct directory
        chdir(join(BASE_PATH, "examples", self._predict_path))

        # save the cwd, because we need to chdir for executing the script
        current_cwd = getcwd()

        # overwrite the last command to the correct script (if we append, we modify config for prediction as well)
        self._config.job_name = "pred_traj"
        self._config.commands[-1] = "python3 predict_trajectories.py -i $1 -n $2 -p $3"
        self._config.write(join(current_cwd, self._script_name))

        pred_id = []
        for m in range(n_pred):
            # random numbers are not unique -> use counter for saving / loading the data by name
            pred_id.append(m)
            self._manager.add(submit_and_wait, [self._script_name, str(m), str(no), self._predict_path])
        self._manager.run()

        # load the trajectories once all processes are done and sort the data
        for j in pred_id:
            res = pt.load(join(current_cwd, f"prediction_no{j}.pt"))
            if self.check_mb_trajectories.check_trajectories(res["pred"]):
                # add the predicted trajectory
                self._predictions.append(self._resort_predictions(res["pred"]))

                # add the predictions made by each model
                res["pred_model"] = [self._resort_predictions(p) for p in res["pred_model"]]
                self.states_model.append(pt.cat([i["states"].unsqueeze(-1) for i in res["pred_model"]], dim=-1))
                self.actions_model.append(pt.cat([i["actions"].unsqueeze(-1) for i in res["pred_model"]], dim=-1))
                self.rewards_model.append(pt.cat([self._sim.reward(i).unsqueeze(-1) for i in res["pred_model"]],
                                                 dim=-1))
                self._counter += 1
            else:
                self._failed_total += 1
                self._n_failed_per_iter += 1
                self.check_mb_trajectories.print_info(self._counter + 1, no)

        # then go back to the 'drlfoam/examples' directory and continue with PPO update
        chdir(join(BASE_PATH, "examples"))

    def _reset(self) -> None:
        """
        reset the buffer, losses and predictions for each model

        :return: None
        """
        self._predictions = []
        self.actions_model = []
        self.states_model = []
        self.rewards_model = []
        self._policy_loss_model = []

    def _reset_slurm(self) -> None:
        """
        remove all created tmp files when executer is SLURM

        :return: None
        """
        [remove(f) for f in glob(join(BASE_PATH, "examples", self._predict_path, f"prediction_no*.pt"))]
        [remove(f) for f in glob(join(BASE_PATH, "examples", self._predict_path, "slurm*.out"))]
        remove(join(BASE_PATH, "examples", self._predict_path, "settings_prediction.pt"))
        remove(join(BASE_PATH, "examples", self._predict_path, "initial_states.pt"))
        remove(join(BASE_PATH, "examples", self._predict_path, self._script_name))

    def _save_for_execution_slurm(self) -> None:
        """
        save all required data in tmp files to load when executing the prediction

        :return: None
        """
        # set up everything for prediction
        pt.save(self._initial_states, join(self._predict_path, "initial_states.pt"))
        pt.save({"train_path": self._predict_path, "env_model": self._model_ensemble,
                 "n_states": self._n_states, "n_input": self._n_t_input, "episode": self._episode,
                 "min_max": self._min_max, "len_traj": self._len_trajectory, "n_actions": self._n_actions,
                 "n_models": self._n_models},
                join(self._predict_path, "settings_prediction.pt"))

    def _resort_predictions(self, data: dict) -> dict:
        """
        split up the actions, cy and cx into the different parts to be consistent with PPO routine

        :param data: dict containing the predicted trajectory
        :return: resorted trajectory
        """
        keys = ["a", "b", "c"]
        params = ["cx", "cy", "alpha", "beta"]
        for k in list(data.keys()):
            if k in params:
                for a in range(self._n_actions):
                    data[f"{k}_{keys[a]}"] = data[k][:, a]
                del data[k]

        # compute rewards
        data["rewards"] = self._sim.reward(data)
        return data


if __name__ == "__main__":
    pass
